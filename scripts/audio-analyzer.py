#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
éŸ³é¢‘è´¨é‡åˆ†æå™¨ v4.1 (å¸¦è¿›åº¦æ¡ä¼˜åŒ–ç‰ˆ) - PyInstallerå…¼å®¹ç‰ˆ
ä¿æŒåŸå§‹è¯„åˆ†ç®—æ³•å®Œæ•´æ€§
"""

import sys
import os

# PyInstallerå…¼å®¹æ€§ä¿®å¤ - åœ¨å…¶ä»–å¯¼å…¥ä¹‹å‰
if getattr(sys, 'frozen', False):
    import multiprocessing
    multiprocessing.freeze_support()

    if sys.platform == 'darwin':
        bundle_dir = getattr(sys, '_MEIPASS', os.path.dirname(os.path.abspath(__file__)))
        os.environ['DYLD_LIBRARY_PATH'] = bundle_dir + ':' + os.environ.get('DYLD_LIBRARY_PATH', '')

try:
    import multiprocessing
    if sys.platform in ['win32', 'darwin']:
        if __name__ == "__main__":
            multiprocessing.set_start_method('spawn', force=True)
except (ImportError, RuntimeError):
    pass

import pandas as pd
import numpy as np
import argparse
import json
import time
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
import logging
import warnings

try:
    from tqdm import tqdm
    HAS_TQDM = True
except ImportError:
    HAS_TQDM = False
    class tqdm:
        def __init__(self, total=None, desc="", bar_format=None):
            self.total = total
            self.desc = desc
            self.current = 0
            print(f"{desc} å¼€å§‹...")

        def update(self, n=1):
            self.current += n
            if self.total:
                progress = (self.current / self.total) * 100
                print(f"{desc} è¿›åº¦: {progress:.1f}%")

        def set_postfix_str(self, s):
            print(f"  {s}")

        def __enter__(self):
            return self

        def __exit__(self, *args):
            print(f"{self.desc} å®Œæˆ!")

warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)
warnings.filterwarnings('ignore', category=UserWarning)

logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)

@dataclass
class QualityThresholds:
    """è´¨é‡è¯„åˆ†é˜ˆå€¼é…ç½®ï¼ˆä¸åŸç‰ˆä¿æŒä¸€è‡´ï¼‰"""
    spectrum_fake_threshold: float = -85.0
    spectrum_processed_threshold: float = -80.0
    spectrum_good_threshold: float = -70.0
    lra_poor_max: float = 3.0
    lra_low_max: float = 6.0
    lra_excellent_min: float = 8.0
    lra_excellent_max: float = 12.0
    lra_acceptable_max: float = 15.0
    lra_too_high: float = 20.0
    peak_clipping_db: float = -0.1
    peak_clipping_linear: float = 0.999
    peak_good_db: float = -6.0
    peak_medium_db: float = -3.0

class AudioQualityAnalyzer:
    """é«˜æ€§èƒ½éŸ³é¢‘è´¨é‡åˆ†æå™¨ï¼ˆPyInstallerå…¼å®¹ç‰ˆ - ä¿æŒåŸå§‹è¯„åˆ†ç®—æ³•ï¼‰"""

    def __init__(self):
        self.thresholds = QualityThresholds()
        self.stats = {
            'total_files': 0,
            'processed_files': 0,
            'processing_time': 0.0
        }

    def _safe_fillna(self, series, value=0):
        """å®‰å…¨çš„fillnaæ“ä½œ"""
        try:
            return series.fillna(value)
        except Exception:
            return series.replace([np.nan, None], value)

    def _map_to_score_vectorized(self, values: pd.Series, in_min: float, in_max: float, out_min: float = 0, out_max: float = 1) -> pd.Series:
        """åŸå§‹çš„åˆ†æ•°æ˜ å°„å‡½æ•° - ä¿æŒä¸å˜"""
        values = self._safe_fillna(values, 0)
        values = np.clip(values, in_min, in_max)
        if in_max == in_min:
            return pd.Series([out_min] * len(values))
        return out_min + (values - in_min) * (out_max - out_min) / (in_max - in_min)

    def _analyze_row_vectorized(self, df: pd.DataFrame) -> Tuple[pd.Series, pd.Series]:
        """åŸå§‹çš„çŠ¶æ€åˆ†æå‡½æ•° - ä¿æŒå®Œå…¨ä¸å˜"""
        status_series = pd.Series(['è´¨é‡è‰¯å¥½'] * len(df))
        notes_series = pd.Series([''] * len(df))

        critical_fields = ['rmsDbAbove18k', 'lra']
        peak_field = None
        if 'peakAmplitudeDb' in df.columns:
            peak_field = 'peakAmplitudeDb'
            critical_fields.append('peakAmplitudeDb')
        elif 'peakAmplitude' in df.columns:
            peak_field = 'peakAmplitude'
            critical_fields.append('peakAmplitude')

        missing_counts = pd.Series([0] * len(df))
        missing_fields_list = []

        for field in critical_fields:
            if field in df.columns:
                field_missing = df[field].isna() | (df[field] == 0.0)
                missing_counts += field_missing.astype(int)
                for idx in df[field_missing].index:
                    if idx not in missing_fields_list:
                        missing_fields_list.append(idx)
            else:
                missing_counts += 1

        incomplete_mask = missing_counts >= 2
        status_series.loc[incomplete_mask] = 'æ•°æ®ä¸å®Œæ•´'
        notes_series.loc[incomplete_mask] = 'å…³é”®æ•°æ®ç¼ºå¤±ï¼Œåˆ†æå¯èƒ½ä¸å‡†ç¡®ã€‚'

        if 'rmsDbAbove18k' in df.columns:
            rms_18k = self._safe_fillna(df['rmsDbAbove18k'], 0)

            fake_mask = (rms_18k < self.thresholds.spectrum_fake_threshold) & (~incomplete_mask)
            status_series.loc[fake_mask] = 'å¯ç–‘ (ä¼ªé€ )'
            notes_series.loc[fake_mask] = 'é¢‘è°±åœ¨çº¦ 18kHz å¤„å­˜åœ¨ç¡¬æ€§æˆªæ­¢ (é«˜åº¦ç–‘ä¼¼ä¼ªé€ /å‡é¢‘)ã€‚'

            processed_mask = (rms_18k < self.thresholds.spectrum_processed_threshold) & (rms_18k >= self.thresholds.spectrum_fake_threshold) & (~incomplete_mask) & (~fake_mask)
            status_series.loc[processed_mask] = 'ç–‘ä¼¼å¤„ç†'
            notes_series.loc[processed_mask] = 'é¢‘è°±åœ¨ 18kHz å¤„èƒ½é‡è¾ƒä½ï¼Œå¯èƒ½å­˜åœ¨è½¯æ€§æˆªæ­¢ã€‚'

        if peak_field and peak_field in df.columns:
            peak_values = self._safe_fillna(df[peak_field], -144.0 if peak_field == 'peakAmplitudeDb' else 0.0)

            if peak_field == 'peakAmplitudeDb':
                clipping_mask = (peak_values >= self.thresholds.peak_clipping_db) & (~incomplete_mask) & (~status_series.str.contains('å¯ç–‘', na=False))
            else:
                clipping_mask = (peak_values >= self.thresholds.peak_clipping_linear) & (~incomplete_mask) & (~status_series.str.contains('å¯ç–‘', na=False))

            status_series.loc[clipping_mask] = 'å·²å‰Šæ³¢'
            notes_series.loc[clipping_mask] = np.where(notes_series.loc[clipping_mask] != '', notes_series.loc[clipping_mask] + ' | å­˜åœ¨ä¸¥é‡æ•°å­—å‰Šæ³¢é£é™©', 'å­˜åœ¨ä¸¥é‡æ•°å­—å‰Šæ³¢é£é™©')

            if peak_field == 'peakAmplitudeDb':
                notes_series.loc[clipping_mask] += ' (å³°å€¼æ¥è¿‘0dB)ã€‚'
            else:
                notes_series.loc[clipping_mask] += 'ã€‚'

        if 'lra' in df.columns:
            lra_values = self._safe_fillna(df['lra'], 0)
            lra_valid = (lra_values > 0) & (~incomplete_mask)

            severe_compression_mask = (lra_values < self.thresholds.lra_poor_max) & lra_valid & (~status_series.str.contains('å¯ç–‘', na=False))
            status_series.loc[severe_compression_mask] = 'ä¸¥é‡å‹ç¼©'
            for idx in df[severe_compression_mask].index:
                lra_val = df.loc[idx, 'lra']
                note = f'åŠ¨æ€èŒƒå›´æä½ (LRA: {lra_val:.1f} LU)ï¼Œä¸¥é‡è¿‡åº¦å‹ç¼©ã€‚'
                if notes_series.loc[idx] != '':
                    notes_series.loc[idx] += f' | {note}'
                else:
                    notes_series.loc[idx] = note

            low_dynamic_mask = (lra_values >= self.thresholds.lra_poor_max) & (lra_values < self.thresholds.lra_low_max) & lra_valid & (~status_series.str.contains('å¯ç–‘|ä¸¥é‡å‹ç¼©|å·²å‰Šæ³¢', na=False))
            status_series.loc[low_dynamic_mask] = 'ä½åŠ¨æ€'
            for idx in df[low_dynamic_mask].index:
                lra_val = df.loc[idx, 'lra']
                note = f'åŠ¨æ€èŒƒå›´è¿‡ä½ (LRA: {lra_val:.1f} LU)ï¼Œå¯èƒ½è¿‡åº¦å‹ç¼©ã€‚'
                if notes_series.loc[idx] != '':
                    notes_series.loc[idx] += f' | {note}'
                else:
                    notes_series.loc[idx] = note

            too_high_mask = (lra_values > self.thresholds.lra_too_high) & lra_valid & (~status_series.str.contains('å¯ç–‘|ä¸¥é‡å‹ç¼©|å·²å‰Šæ³¢|ä½åŠ¨æ€', na=False))
            for idx in df[too_high_mask].index:
                lra_val = df.loc[idx, 'lra']
                note = f'åŠ¨æ€èŒƒå›´è¿‡é«˜ (LRA: {lra_val:.1f} LU)ï¼Œå¯èƒ½éœ€è¦å‹ç¼©å¤„ç†ã€‚'
                if notes_series.loc[idx] != '':
                    notes_series.loc[idx] += f' | {note}'
                else:
                    notes_series.loc[idx] = note

        default_mask = notes_series == ''
        notes_series.loc[default_mask] = 'æœªå‘ç°æ˜æ˜¾çš„ç¡¬æ€§æŠ€æœ¯é—®é¢˜ã€‚'

        return status_series, notes_series

    def _calculate_quality_score_vectorized(self, df: pd.DataFrame) -> pd.Series:
        """åŸå§‹çš„è´¨é‡è¯„åˆ†å‡½æ•° - å®Œå…¨æ¢å¤åŸç®—æ³•"""
        MAX_SCORE_INTEGRITY, MAX_SCORE_DYNAMICS, MAX_SCORE_SPECTRUM = 40, 30, 30

        integrity_scores = pd.Series([0.0] * len(df))
        dynamics_scores = pd.Series([0.0] * len(df))
        spectrum_scores = pd.Series([0.0] * len(df))

        critical_fields = ['rmsDbAbove18k', 'lra']
        peak_field = None
        if 'peakAmplitudeDb' in df.columns:
            peak_field = 'peakAmplitudeDb'
            critical_fields.append('peakAmplitudeDb')
        elif 'peakAmplitude' in df.columns:
            peak_field = 'peakAmplitude'
            critical_fields.append('peakAmplitude')

        completeness_penalty = pd.Series([0] * len(df))
        for field in critical_fields:
            if field in df.columns:
                completeness_penalty += (df[field].isna() | (df[field] == 0.0)).astype(int) * 10
            else:
                completeness_penalty += 10

        if 'rmsDbAbove18k' in df.columns:
            rms_18k = self._safe_fillna(df['rmsDbAbove18k'], 0)
            valid_rms = rms_18k != 0

            excellent_mask = (rms_18k >= self.thresholds.spectrum_good_threshold) & valid_rms
            integrity_scores.loc[excellent_mask] += 25

            good_mask = (rms_18k >= self.thresholds.spectrum_processed_threshold) & (rms_18k < self.thresholds.spectrum_good_threshold) & valid_rms
            integrity_scores.loc[good_mask] += self._map_to_score_vectorized(rms_18k.loc[good_mask], self.thresholds.spectrum_processed_threshold, self.thresholds.spectrum_good_threshold, 15, 25)

            medium_mask = (rms_18k >= self.thresholds.spectrum_fake_threshold) & (rms_18k < self.thresholds.spectrum_processed_threshold) & valid_rms
            integrity_scores.loc[medium_mask] += self._map_to_score_vectorized(rms_18k.loc[medium_mask], self.thresholds.spectrum_fake_threshold, self.thresholds.spectrum_processed_threshold, 5, 15)

        if peak_field and peak_field in df.columns:
            peak_values = self._safe_fillna(df[peak_field], -144.0 if peak_field == 'peakAmplitudeDb' else 0.0)
            valid_peak = ~df[peak_field].isna()

            if peak_field == 'peakAmplitudeDb':
                excellent_mask = (peak_values <= self.thresholds.peak_good_db) & valid_peak
                integrity_scores.loc[excellent_mask] += 15

                good_mask = (peak_values > self.thresholds.peak_good_db) & (peak_values <= self.thresholds.peak_medium_db) & valid_peak
                integrity_scores.loc[good_mask] += self._map_to_score_vectorized(peak_values.loc[good_mask], self.thresholds.peak_good_db, self.thresholds.peak_medium_db, 15, 10)

                medium_mask = (peak_values > self.thresholds.peak_medium_db) & (peak_values <= self.thresholds.peak_clipping_db) & valid_peak
                integrity_scores.loc[medium_mask] += self._map_to_score_vectorized(peak_values.loc[medium_mask], self.thresholds.peak_medium_db, self.thresholds.peak_clipping_db, 10, 3)
            else:
                excellent_mask = (peak_values <= 0.5) & valid_peak
                integrity_scores.loc[excellent_mask] += 15

                good_mask = (peak_values > 0.5) & (peak_values <= 0.8) & valid_peak
                integrity_scores.loc[good_mask] += self._map_to_score_vectorized(peak_values.loc[good_mask], 0.5, 0.8, 15, 10)

                medium_mask = (peak_values > 0.8) & (peak_values <= 0.999) & valid_peak
                integrity_scores.loc[medium_mask] += self._map_to_score_vectorized(peak_values.loc[medium_mask], 0.8, 0.999, 10, 3)

        if 'lra' in df.columns:
            lra_values = self._safe_fillna(df['lra'], 0)
            valid_lra = lra_values > 0

            ideal_mask = (lra_values >= self.thresholds.lra_excellent_min) & (lra_values <= self.thresholds.lra_excellent_max) & valid_lra
            dynamics_scores.loc[ideal_mask] = 30

            low_acceptable_mask = (lra_values >= self.thresholds.lra_low_max) & (lra_values < self.thresholds.lra_excellent_min) & valid_lra
            dynamics_scores.loc[low_acceptable_mask] = self._map_to_score_vectorized(lra_values.loc[low_acceptable_mask], self.thresholds.lra_low_max, self.thresholds.lra_excellent_min, 20, 28)

            high_mask = (lra_values > self.thresholds.lra_excellent_max) & (lra_values <= self.thresholds.lra_acceptable_max) & valid_lra
            dynamics_scores.loc[high_mask] = self._map_to_score_vectorized(lra_values.loc[high_mask], self.thresholds.lra_excellent_max, self.thresholds.lra_acceptable_max, 28, 22)

            low_mask = (lra_values >= self.thresholds.lra_poor_max) & (lra_values < self.thresholds.lra_low_max) & valid_lra
            dynamics_scores.loc[low_mask] = self._map_to_score_vectorized(lra_values.loc[low_mask], self.thresholds.lra_poor_max, self.thresholds.lra_low_max, 10, 20)

            very_low_mask = (lra_values < self.thresholds.lra_poor_max) & valid_lra
            dynamics_scores.loc[very_low_mask] = self._map_to_score_vectorized(lra_values.loc[very_low_mask], 0, self.thresholds.lra_poor_max, 0, 10)

            too_high_mask = (lra_values > self.thresholds.lra_acceptable_max) & valid_lra
            dynamics_scores.loc[too_high_mask] = 18

        if 'rmsDbAbove16k' in df.columns:
            rms_16k = self._safe_fillna(df['rmsDbAbove16k'], -90)
            spectrum_scores = self._map_to_score_vectorized(rms_16k, -90, -55, 0, 30)

        total_scores = integrity_scores + dynamics_scores + spectrum_scores - completeness_penalty

        if 'çŠ¶æ€' in df.columns:
            fake_mask = df['çŠ¶æ€'] == 'å¯ç–‘ (ä¼ªé€ )'
            total_scores.loc[fake_mask] = np.minimum(total_scores.loc[fake_mask], 20)

            incomplete_mask = df['çŠ¶æ€'] == 'æ•°æ®ä¸å®Œæ•´'
            total_scores.loc[incomplete_mask] = np.minimum(total_scores.loc[incomplete_mask], 40)

        return np.maximum(0, total_scores.round()).astype(int)

    def analyze_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """åˆ†æå®Œæ•´çš„DataFrame"""
        if df.empty:
            logger.warning("è¾“å…¥DataFrameä¸ºç©º")
            return df

        self.stats['total_files'] = len(df)
        logger.info("-" * 40)
        logger.info(f"Pythonåˆ†ææ¨¡å—å¯åŠ¨ï¼Œå…± {len(df)} ä¸ªæ–‡ä»¶å¾…å¤„ç†ã€‚")
        logger.info("-" * 40)

        start_time = time.time()

        with tqdm(total=3, desc="[ Python ç«¯åˆ†æè¿›åº¦ ]", bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt}") as pbar:
            pbar.set_postfix_str("Step 1: åˆ†æçŠ¶æ€ä¸å¤‡æ³¨...")
            status_series, notes_series = self._analyze_row_vectorized(df)
            df['çŠ¶æ€'] = status_series
            df['å¤‡æ³¨'] = notes_series
            time.sleep(0.1)
            pbar.update(1)

            pbar.set_postfix_str("Step 2: è®¡ç®—ç»¼åˆè´¨é‡åˆ†...")
            df['è´¨é‡åˆ†'] = self._calculate_quality_score_vectorized(df)
            time.sleep(0.1)
            pbar.update(1)

            pbar.set_postfix_str("Step 3: æ ¼å¼åŒ–ä¸æ’åº...")
            report_df = self.format_output_dataframe(df)
            time.sleep(0.1)
            pbar.update(1)

            pbar.set_postfix_str("åˆ†æå®Œæˆ!")

        self.stats['processing_time'] = time.time() - start_time
        self.stats['processed_files'] = len(df)

        logger.info(f"Python ç«¯åˆ†æå®Œæˆï¼Œè€—æ—¶ {self.stats['processing_time']:.2f} ç§’")
        logger.info("-" * 40)

        return report_df

    def format_output_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ ¼å¼åŒ–è¾“å‡ºDataFrame"""
        peak_field = None
        if 'peakAmplitudeDb' in df.columns:
            peak_field = 'peakAmplitudeDb'
        elif 'peakAmplitude' in df.columns:
            peak_field = 'peakAmplitude'

        output_columns = ['è´¨é‡åˆ†', 'çŠ¶æ€', 'filePath', 'å¤‡æ³¨', 'lra']
        if peak_field:
            output_columns.append(peak_field)

        additional_fields = ['rmsDbAbove16k', 'rmsDbAbove18k', 'rmsDbAbove20k', 'overallRmsDb']
        for field in additional_fields:
            if field in df.columns:
                output_columns.append(field)

        final_columns = [col for col in output_columns if col in df.columns]
        result_df = df[final_columns].copy()
        result_df = result_df.sort_values(by='è´¨é‡åˆ†', ascending=False)

        return result_df

def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="åˆ†æç”± audio_analyzer (Rust) ç”Ÿæˆçš„ JSON æ•°æ® (v4.1 PyInstallerå…¼å®¹ç‰ˆ)ã€‚"
    )

    parser.add_argument("input_json", help="è¾“å…¥çš„ analysis_data.json æ–‡ä»¶è·¯å¾„ã€‚")
    parser.add_argument("-o", "--output", default="audio_quality_report_v4.csv",
                       help="è¾“å‡ºçš„ CSV æŠ¥å‘Šæ–‡ä»¶åã€‚")
    parser.add_argument("--min-score", type=int, default=0,
                       help="åªæ˜¾ç¤ºé«˜äºæŒ‡å®šåˆ†æ•°çš„æ–‡ä»¶ (é»˜è®¤: 0)ã€‚")
    parser.add_argument("--show-incomplete", action="store_true",
                       help="æ˜¾ç¤ºæ•°æ®ä¸å®Œæ•´çš„æ–‡ä»¶è¯¦æƒ…ã€‚")
    parser.add_argument("--show-stats", action="store_true",
                       help="æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡ä¿¡æ¯ã€‚")

    args = parser.parse_args()

    if not os.path.exists(args.input_json):
        print(f"é”™è¯¯: è¾“å…¥æ–‡ä»¶ '{args.input_json}' ä¸å­˜åœ¨ã€‚", file=sys.stderr)
        return 1

    try:
        df = pd.read_json(args.input_json)
    except Exception as e:
        print(f"é”™è¯¯: æ— æ³•è§£æJSONæ–‡ä»¶: {e}", file=sys.stderr)
        return 1

    if df.empty:
        print("JSON æ–‡ä»¶ä¸ºç©ºï¼Œæ²¡æœ‰å¯åˆ†æçš„æ•°æ®ã€‚")
        return 0

    try:
        analyzer = AudioQualityAnalyzer()
        report_df = analyzer.analyze_dataframe(df)

        if args.min_score > 0:
            original_count = len(report_df)
            report_df = report_df[report_df['è´¨é‡åˆ†'] >= args.min_score]
            filtered_count = original_count - len(report_df)
            if filtered_count > 0:
                print(f"å·²è¿‡æ»¤æ‰ {filtered_count} ä¸ªä½åˆ†æ–‡ä»¶ (< {args.min_score}åˆ†)")

        report_df.to_csv(args.output, index=False, encoding='utf-8-sig')
        print(f"\nâœ… å®Œæ•´çš„åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {args.output}")
        if len(report_df) < len(df):
            filtered_count = len(df) - len(report_df)
            print(f" (å·²è¿‡æ»¤æ‰ {filtered_count} ä¸ªä½åˆ†æ–‡ä»¶)")

        print(f"\n--- ä¼˜åŒ–åˆ†ææ‘˜è¦ (v4.1) ---")
        status_counts = report_df['çŠ¶æ€'].value_counts()
        print(f"\nğŸ“Š è´¨é‡çŠ¶æ€åˆ†å¸ƒ:")
        for status, count in status_counts.items():
            percentage = (count / len(df)) * 100
            print(f" - {status}: {count} ä¸ªæ–‡ä»¶ ({percentage:.1f}%)")

        print(f"\nğŸ† è´¨é‡æ’åå‰ 5 çš„æ–‡ä»¶:")
        for i, (_, row) in enumerate(report_df.head(5).iterrows(), 1):
            filename = os.path.basename(row['filePath']) if 'filePath' in row else 'Unknown'
            print(f" {i}. [åˆ†æ•°: {int(row['è´¨é‡åˆ†'])}] {filename}")

        return 0

    except Exception as e:
        print(f"åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {e}", file=sys.stderr)
        return 1

if __name__ == "__main__":
    if getattr(sys, 'frozen', False):
        multiprocessing.freeze_support()

    sys.exit(main())
